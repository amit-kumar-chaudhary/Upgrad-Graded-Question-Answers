#!/usr/bin/env python
# coding: utf-8

# **Q1 - Gradient Boosting**
# 
# Consider the following statements w.r.t Gradient Boosting and choose the correct one:
# 
# 1. At each iteration, we add an incremental model, which is fitted on the positive gradients of the loss function evaluated at current target values.
# 
# 2. We multiply λt(learning rate) with the incremental model ht+1 so that the new model does not overfit.
# 
# - Only 1
# 
# 
# - **Only 2**
# 
#         - At each iteration, we add an incremental model ht+1, which fits on the negative gradients of the loss function evaluated at current target values. But to generate the final model we multiply λt with the incremental model ht+1 so that the new model doesn't  overfit
# 
# 
# - Both 1 & 2
# 
# 
# - None of the above

# **Q2 - Adaboost**
# 
# In Adaboost, each model has different say/importance according to the error it has made while predicting the training data.
# 
# This is depicted in the following equation: α =  0.5 ln((1-TOTAL ERROR)/(TOTAL ERROR))
# 
# With respect to this, consider the following statements and choose the correct:
# 
# 1. The classifier weight grows exponentially as the error approaches 0. Better classifiers are given exponentially more weight.
# 2. The classifier weight is 0.5 if the error rate is 0.5. This is because the classifier has only 50% accuracy. 
# 3. The classifier weight grows exponentially negative as the error approaches 1. These types of classifiers are given a negative weight.
# 
# - 1 & 2 only
# 
# 
# - **1 & 3 only** 
# 
#         - The classifier weight is zero if the error rate is 0.5. A classifier with 50% accuracy is no better than random guessing, so we ignore it.
# 
# 
# - 2 & 3 only
# 
# 
# - All of the above

# **Q3 - General Expression for Residual**
# 
# For a gradient boosting algorithm, let's say F0 is the crude model with which we start off. For a model Ft which is fitted on the training data, the prediction we get for xi is Ft(xi). What is the general expression of the residuals generated once the Ft model is trained? Assume y is the initial target variable.
# 
# 
# 1. y - [Fo(xi)+F1(xi) + F2 (xi)+.........+ Ft−2(xi) + Ft−1(xi)]
# 
# 
# 2. **y - [F0(xi)+Ft−1(xi)]**
# 
#        - The residuals created by F1 is y−F0(xi)−F1(xi), for F2 it is  y−F0(xi)−F1(xi) - F2(xi) and so on.
# 
# 
# 3. **y - [Fo(xi)+F1(xi) + F2 (xi)+.........+ Ft−2(xi) + Ft−1(xi) + Ft(xi)]**
# 
#         - The residuals created by F1 is y−F0(xi)−F1(xi), for F2 it is  y−F0(xi)−F1(xi) - F2(xi) and so on. In general, Ft trains on the residuals generated by the model Ft−1.  So the residuals created by Ft is y - [Fo(xi)+F1(xi) + F2 (xi)+.........+ Ft−2(xi) + Ft−1(xi) + Ft(xi)].

# **Q4 - XGBoost**
# 
# Which of these features are the advantages of XGBoost algorithm?
# 
# More than one option can be correct.
# 
# 
# 1. **Parallel and distributed computing**
# 
#         - Fast learning through parallel and distributed computing enables quicker model exploration.
# 
# 
# 2. **Handling of missing values** 
# 
#         - XGBoost handles missing values by treating thenm in such a manner that any trend in missing values (if it exists)  is captured by the model.
# 
# 
# 3. **Regularisation**
# 
#         - Regularisation  is added in the objective function of XGBoost to penalize the model based on the number of trees and the depth of the model
